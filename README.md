# Airflow ETL Pipeline with PostgreSQL & API Integration

## ğŸ“Œ Project Overview
This project implements an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** to:
- Extract data from an **external API**.
- Transform the data for preprocessing.
- Load the processed data into a **PostgreSQL database**.

The project follows the **Astro CLI structure** (`astro dev init`).

---

## ğŸ— Project Structure
```
.
â”œâ”€â”€ dags/                # Airflow DAGs (Pipeline definitions)
â”‚   â”œâ”€â”€ etl_pipeline.py  # Main ETL workflow
â”‚   â”œâ”€â”€ utils.py         # Utility functions (API calls, transformations)
â”‚
â”œâ”€â”€ include/             # Additional files (SQL scripts, configurations)
â”‚
â”œâ”€â”€ plugins/             # Custom Airflow plugins (if needed)
â”‚
â”œâ”€â”€ tests/               # Unit tests for DAGs
â”‚
â”œâ”€â”€ Dockerfile           # Airflow Docker setup
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ airflow_settings.yaml # Airflow configurations
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ .astro/              # Astro CLI configurations
```

---

## âš™ï¸ Prerequisites
- **Docker & Docker Compose** installed
- **Astro CLI** installed (`pip install astro-cli`)
- **PostgreSQL Database** (running locally or on cloud)

---

## ğŸš€ Getting Started
### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-repo/airflow-etl-pipeline.git
cd airflow-etl-pipeline
```

### 2ï¸âƒ£ Start Airflow
```bash
astro dev start
```

### 3ï¸âƒ£ Set up PostgreSQL Connection in Airflow UI
1. Open Airflow UI (`http://localhost:8080`)
2. Go to **Admin â†’ Connections**
3. Create a **Postgres Connection** with:
   - Conn ID: `postgres_db`
   - Conn Type: `Postgres`
   - Host: `localhost`
   - Schema: `your_database`
   - Login: `your_user`
   - Password: `your_password`

### 4ï¸âƒ£ Trigger the DAG
1. Open Airflow UI (`http://localhost:8080`)
2. Enable and run the DAG `etl_pipeline`

---

## ğŸ”§ Key Components
- **DAG (`etl_pipeline.py`)**: Defines the ETL workflow.
- **Task 1 - Extract**: Fetches data from an API.
- **Task 2 - Transform**: Cleans and processes data.
- **Task 3 - Load**: Inserts data into PostgreSQL.

---

## ğŸ“œ License
MIT License

---

## ğŸ“¬ Contact
For questions or contributions, reach out via [GitHub Issues](https://github.com/your-repo/airflow-etl-pipeline/issues).

