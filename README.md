# Airflow ETL Pipeline with PostgreSQL and OpenWeather API

## ğŸ“Œ Project Overview
This project implements an **ETL (Extract, Transform, Load) pipeline** using **Apache Airflow** to:
- Extract weather data from the **OpenWeather API**.
- Transform the data for preprocessing.
- Load the processed data into a **PostgreSQL database**.

The project follows the **Astro CLI structure** (`astro dev init`).

---

## ğŸ— Project Structure
```
.
â”œâ”€â”€ dags/                # Airflow DAGs (Pipeline definitions)
â”‚   â”œâ”€â”€ etl_pipeline.py  # Main ETL workflow
â”‚   â”œâ”€â”€ utils.py         # Utility functions (API calls, transformations)
â”‚
â”œâ”€â”€ include/             # Additional files (SQL scripts, configurations)
â”‚
â”œâ”€â”€ plugins/             # Custom Airflow plugins (if needed)
â”‚
â”œâ”€â”€ tests/               # Unit tests for DAGs
â”‚
â”œâ”€â”€ Dockerfile           # Airflow Docker setup
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ airflow_settings.yaml # Airflow configurations
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ .astro/              # Astro CLI configurations
```

---

## ğŸ› ï¸ Tools Used
### **Apache Airflow**
- A powerful workflow orchestration tool for scheduling and managing data pipelines.
- Provides DAGs (Directed Acyclic Graphs) to define ETL workflows.

### **PostgreSQL**
- A robust, open-source relational database system.
- Used to store the transformed weather data.

### **OpenWeather API**
- Provides real-time and historical weather data.
- Free access available with API key registration.

---

## âš™ï¸ Prerequisites
- **Docker & Docker Compose** installed
- **Astro CLI** installed (`pip install astro-cli`)
- **PostgreSQL Database** (running locally or on cloud)
- **OpenWeather API Key** (Get from [OpenWeather](https://openweathermap.org/api))

---

## ğŸš€ Getting Started
### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/ETL-pipeline-airflow-to-postgres.git
cd ETL-pipeline-airflow-to-postgres
```

### 2ï¸âƒ£ Start Airflow
```bash
astro dev start
```

### 3ï¸âƒ£ Set up PostgreSQL Connection in Airflow UI
1. Open Airflow UI (`http://localhost:8080`)
2. Go to **Admin â†’ Connections**
3. Create a **Postgres Connection** with:
   - Conn ID: `postgres_db`
   - Conn Type: `Postgres`
   - Host: `localhost`
   - Schema: `your_database`
   - Login: `your_user`
   - Password: `your_password`

### 4ï¸âƒ£ Add OpenWeather API Key
- Store your **API key** in Airflow Variables:
  ```bash
  airflow variables set openweather_api_key "YOUR_API_KEY"
  ```

### 5ï¸âƒ£ Trigger the DAG
1. Open Airflow UI (`http://localhost:8080`)
2. Enable and run the DAG `etl_pipeline`

---

## ğŸ”§ Key Components
- **DAG (`etl_pipeline.py`)**: Defines the ETL workflow.
- **Task 1 - Extract**: Fetches weather data from OpenWeather API.
- **Task 2 - Transform**: Cleans and processes data.
- **Task 3 - Load**: Inserts data into PostgreSQL.

---

## ğŸ“œ License
MIT License

---

## ğŸ“¬ Contact
For questions or contributions, reach out via [GitHub Issues](https://github.com/your-username/ETL-pipeline-airflow-to-postgres/issues).

